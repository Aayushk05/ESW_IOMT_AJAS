{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Installing Libraries ---\")\n",
    "# Install Gradio and PyArrow. Tensorflow is pre-installed\n",
    "!pip install gradio pyarrow -q\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "import gc\n",
    "import shutil\n",
    "print(\"\\n--- Cleaning up previous data... ---\")\n",
    "!rm -rf /content/fhir/\n",
    "!rm -rf /content/output/\n",
    "!rm -rf /content/temp_parquet/\n",
    "!rm -f /content/fhir.zip  # We only need to clean up the zip file\n",
    "# Create the target directory\n",
    "os.makedirs('/content/fhir', exist_ok=True)\n",
    "print(\"\\n--- Mounting Google Drive... ---\")\n",
    "drive.mount('/content/drive', force_remount=True) # Force remount to be safe\n",
    "print(\"\\n--- Copying fhir.zip from Drive to Colab's local disk... ---\")\n",
    "drive_zip_path = '/content/drive/My Drive/fhir.zip'\n",
    "local_zip_path = '/content/fhir.zip'\n",
    "\n",
    "if not os.path.exists(drive_zip_path):\n",
    "    print(f\"--- WARNING: {drive_zip_path} not found in Google Drive. Stopping. ---\")\n",
    "else:\n",
    "    print(f\"Copying {drive_zip_path}...\")\n",
    "    !cp \"{drive_zip_path}\" \"{local_zip_path}\"\n",
    "    print(f\"Copy complete: {local_zip_path}\")\n",
    "print(\"\\n--- Unpacking local fhir.zip... ---\")\n",
    "# This is the path to the file we just copied\n",
    "local_zip_path = '/content/fhir.zip'\n",
    "\n",
    "if os.path.exists(local_zip_path):\n",
    "    print(f\"Extracting {local_zip_path}...\")\n",
    "    with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.filelist\n",
    "        # Find all non-empty JSON files within the zip\n",
    "        json_files = [f for f in file_list if f.filename.endswith('.json') and f.file_size > 0]\n",
    "        print(f\"Found {len(json_files)} non-empty JSON files in zip.\")\n",
    "\n",
    "        # Extract only the non-empty JSON files\n",
    "        for i, file_info in enumerate(json_files):\n",
    "            if i % 5000 == 0:\n",
    "                print(f\"    Extracting file {i}/{len(json_files)}...\")\n",
    "            zip_ref.extract(file_info, '/content/') # Extract to /content/\n",
    "\n",
    "    print(f\"--- Finished processing: {local_zip_path} ---\")\n",
    "    # Clean up the local archive to save disk space\n",
    "    os.remove(local_zip_path)\n",
    "    print(f\"Removed local archive: {local_zip_path}\")\n",
    "else:\n",
    "    print(f\"--- WARNING: {local_zip_path} was not found. Skipping extraction. ---\")\n",
    "print(\"\\n--- All archives processed! Consolidating files... ---\")\n",
    "# This logic is important as fhir.zip often extracts into a nested folder.\n",
    "# This code finds that nested folder and moves all files to /content/fhir/\n",
    "\n",
    "if os.path.exists('/content/output/fhir'):\n",
    "    print(\"Consolidating from /content/output/fhir...\")\n",
    "    !mv /content/output/fhir/* /content/fhir/\n",
    "    !rm -rf /content/output\n",
    "\n",
    "# Also check if files were extracted inside a 'fhir' folder (e.g., /content/fhir/fhir/)\n",
    "if os.path.exists('/content/fhir/fhir'):\n",
    "    print(\"Consolidating from /content/fhir/fhir...\")\n",
    "    !mv /content/fhir/fhir/* /content/fhir/\n",
    "    !rm -rf /content/fhir/fhir\n",
    "\n",
    "if os.path.exists('/content/fhir'):\n",
    "    print(\"Final 'fhir' directory is ready. Checking file count...\")\n",
    "    # Count only the JSON files that will be processed\n",
    "    file_count = len([f for f in os.listdir('/content/fhir') if f.endswith('.json')])\n",
    "    print(f\"Total .json files in /content/fhir: {file_count}\")\n",
    "else:\n",
    "    print(\"--- ERROR: Final 'fhir' directory not found. Please check archive contents. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a20c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "# --- NEW: A robust mapping of standard codes to our simple text flags ---\n",
    "CONDITION_CODE_MAP = {\n",
    "    # ICD-10 Codes\n",
    "    'I10': 'hypertension', 'E11': 'diabetes', 'I50': 'congestive heart failure', 'J44': 'copd', 'D64': 'anemia',\n",
    "    'E78': 'hyperlipidemia', 'I25': 'coronary artery disease', 'N18': 'chronic kidney disease', 'E66': 'obesity',\n",
    "    'J45': 'asthma', 'J18': 'pneumonia', 'J11': 'influenza', 'I48': 'atrial fibrillation',\n",
    "    'I82': 'dvt', 'I26': 'pulmonary embolism', 'E03': 'hypothyroidism', 'E05': 'hyperthyroidism',\n",
    "    'M10': 'gout', 'M19': 'osteoarthritis', 'M06': 'rheumatoid arthritis', 'K85': 'pancreatitis',\n",
    "    'K92': 'gi bleed', 'K50': 'crohn\\'s disease', 'K51': 'ulcerative colitis', 'L03': 'cellulitis',\n",
    "    'N39': 'urinary tract infection', 'G40': 'epilepsy', 'R56': 'seizure',\n",
    "    'G30': 'alzheimer\\'s', 'F03': 'dementia', 'F41': 'anxiety', 'F32': 'depression',\n",
    "\n",
    "    # --- NEWLY ADDED 14 CONDITIONS (ICD-10) ---\n",
    "    'K21': 'gerd', 'K21.9': 'gerd',\n",
    "    'J30': 'allergies', 'J30.9': 'allergies',\n",
    "    'G43': 'migraine', 'G43.9': 'migraine',\n",
    "    'G47.33': 'sleep apnea',\n",
    "    'M54.5': 'low back pain',\n",
    "    'F17.2': 'nicotine dependence',\n",
    "    'F10.2': 'alcohol use disorder',\n",
    "    'L20': 'atopic dermatitis', 'L20.9': 'atopic dermatitis',\n",
    "    'D50.9': 'iron deficiency', 'D50.8': 'iron deficiency',\n",
    "    'M81.0': 'osteoporosis',\n",
    "    'G47.00': 'insomnia',\n",
    "    'K58.0': 'ibs', 'K58.9': 'ibs',\n",
    "    'J32.9': 'sinusitis', 'J32.4': 'sinusitis',\n",
    "    'R42': 'dizziness', 'H81.10': 'vertigo',\n",
    "\n",
    "    # SNOMED CT Codes\n",
    "    '59621000': 'hypertension', '44054006': 'diabetes', '42343007': 'congestive heart failure',\n",
    "    '13645005': 'copd', '271737000': 'anemia', '55822004': 'hyperlipidemia', '53741008': 'coronary artery disease',\n",
    "    '709044004': 'chronic kidney disease', '414915002': 'obesity', '195967001': 'asthma',\n",
    "    '233604007': 'pneumonia', '6142004': 'influenza', '49436004': 'atrial fibrillation',\n",
    "    '128063001': 'dvt', '103228002': 'pulmonary embolism', '40930008': 'hypothyroidism', '42725007': 'hyperthyroidism',\n",
    "    '90560007': 'gout', '396275006': 'osteoarthritis', '69896004': 'rheumatoid arthritis', '71831004': 'pancreatitis',\n",
    "    '74474003': 'gi bleed', '34000006': 'crohn\\'s disease', '64766004': 'ulcerative colitis', '23131000': 'cellulitis',\n",
    "    '68566005': 'urinary tract infection', '84757009': 'epilepsy', '91175000': 'seizure', '26929004': 'alzheimer\\'s',\n",
    "    '52448006': 'dementia', '48694002': 'anxiety', '35489007': 'depression', '91302008': 'sepsis',\n",
    "\n",
    "    # --- NEWLY ADDED 14 CONDITIONS (SNOMED) ---\n",
    "    '69808002': 'gerd',\n",
    "    '61598001': 'allergies',\n",
    "    '3860009': 'migraine',\n",
    "    '73430006': 'sleep apnea',\n",
    "    '279039007': 'low back pain',\n",
    "    '77176002': 'nicotine dependence',\n",
    "    '65363002': 'alcohol use disorder',\n",
    "    '24079001': 'atopic dermatitis',\n",
    "    '34443006': 'iron deficiency',\n",
    "    '64859006': 'osteoporosis',\n",
    "    '193462001': 'insomnia',\n",
    "    '44796009': 'ibs',\n",
    "    '40055000': 'sinusitis',\n",
    "    '48340000': 'vertigo', '271789005': 'dizziness'\n",
    "}\n",
    "# --- END OF MODIFICATIONS ---\n",
    "\n",
    "\n",
    "def parse_fhir_bundle_24h(file_path):\n",
    "    try:\n",
    "        if os.path.getsize(file_path) == 0: return None\n",
    "        with open(file_path, 'r') as f: bundle = json.load(f)\n",
    "        if not isinstance(bundle, dict) or 'entry' not in bundle: return None\n",
    "\n",
    "        patient_id = os.path.basename(file_path).split('.')[0]\n",
    "        patient_features = {'patient_id': patient_id}\n",
    "        observations, conditions, medications, procedures = [], set(), set(), set()\n",
    "\n",
    "        for entry in bundle.get('entry', []):\n",
    "            resource = entry.get('resource', {})\n",
    "            r_type = resource.get('resourceType')\n",
    "\n",
    "            if r_type == 'Patient':\n",
    "                patient_features['gender'] = resource.get('gender')\n",
    "                birth_date = resource.get('birthDate')\n",
    "                if birth_date:\n",
    "                    try: patient_features['age'] = (datetime(2025, 1, 1) - datetime.strptime(birth_date, '%Y-%m-%d')).days // 365\n",
    "                    except: patient_features['age'] = None\n",
    "\n",
    "            elif r_type == 'Condition':\n",
    "                text = resource.get('code', {}).get('text', '').lower()\n",
    "                if text: conditions.add(text)\n",
    "                for coding in resource.get('code', {}).get('coding', []):\n",
    "                    code = coding.get('code')\n",
    "                    if code in CONDITION_CODE_MAP:\n",
    "                        conditions.add(CONDITION_CODE_MAP[code])\n",
    "\n",
    "            elif r_type == 'Observation':\n",
    "                obs_time = resource.get('effectiveDateTime')\n",
    "                if obs_time: observations.append((datetime.fromisoformat(obs_time.replace('Z', '+00:00')), resource))\n",
    "\n",
    "            elif r_type == 'MedicationRequest':\n",
    "                text = resource.get('medicationCodeableConcept', {}).get('text', '').lower()\n",
    "                if text: medications.add(text)\n",
    "                for coding in resource.get('medicationCodeableConcept', {}).get('coding', []):\n",
    "                    text = coding.get('display', '').lower()\n",
    "                    if text: medications.add(text)\n",
    "\n",
    "            elif r_type == 'Procedure':\n",
    "                text = resource.get('code', {}).get('text', '').lower()\n",
    "                if text: procedures.add(text)\n",
    "                for coding in resource.get('code', {}).get('coding', []):\n",
    "                    text = coding.get('display', '').lower()\n",
    "                    if text: procedures.add(text)\n",
    "\n",
    "        if not observations: return None\n",
    "        observations.sort(key=lambda x: x[0])\n",
    "\n",
    "        # --- Static Feature Calculation (Efficient) ---\n",
    "        patient_features['history_hypertension'] = int(any('hypertension' in c for c in conditions))\n",
    "        patient_features['history_diabetes'] = int(any('diabetes' in c for c in conditions))\n",
    "        patient_features['history_chf'] = int(any('congestive heart failure' in c for c in conditions))\n",
    "        patient_features['history_copd'] = int(any('copd' in c for c in conditions))\n",
    "        patient_features['history_anemia'] = int(any('anemia' in c for c in conditions))\n",
    "        patient_features['history_hyperlipidemia'] = int(any('hyperlipidemia' in c for c in conditions))\n",
    "        patient_features['history_cad'] = int(any('coronary artery disease' in c for c in conditions))\n",
    "        patient_features['history_ckd'] = int(any('chronic kidney disease' in c for c in conditions))\n",
    "        patient_features['history_obesity'] = int(any('obesity' in c for c in conditions))\n",
    "        patient_features['history_asthma'] = int(any('asthma' in c for c in conditions))\n",
    "        patient_features['history_pneumonia'] = int(any('pneumonia' in c for c in conditions))\n",
    "        patient_features['history_flu'] = int(any('influenza' in c for c in conditions))\n",
    "        patient_features['on_statin'] = int(any('statin' in m for m in medications))\n",
    "        patient_features['on_metformin'] = int(any('metformin' in m for m in medications))\n",
    "        patient_features['on_aspirin'] = int(any('aspirin' in m for m in medications))\n",
    "        patient_features['on_lisinopril'] = int(any('lisinopril' in m for m in medications))\n",
    "        patient_features['history_cabg'] = int(any('cabg' in p or 'coronary artery bypass' in p for p in procedures))\n",
    "        patient_features['history_appendectomy'] = int(any('appendectomy' in p for p in procedures))\n",
    "        patient_features['history_afib'] = int(any('atrial fibrillation' in c for c in conditions))\n",
    "        patient_features['history_dvt_pe'] = int(any(s in c for s in ['dvt', 'deep vein thrombosis', 'pulmonary embolism'] for c in conditions))\n",
    "        patient_features['history_thyroid'] = int(any(s in c for s in ['hypothyroidism', 'hyperthyroidism', 'thyroid disease'] for c in conditions))\n",
    "        patient_features['history_gout'] = int(any('gout' in c for c in conditions))\n",
    "        patient_features['history_arthritis'] = int(any(s in c for s in ['osteoarthritis', 'rheumatoid arthritis'] for c in conditions))\n",
    "        patient_features['history_pancreatitis'] = int(any('pancreatitis' in c for c in conditions))\n",
    "        patient_features['history_gi_bleed'] = int(any(s in c for s in ['gi bleed', 'gastrointestinal bleed', 'varices'] for c in conditions))\n",
    "        patient_features['history_ibd'] = int(any(s in c for s in [\"crohn's disease\", 'ulcerative colitis'] for c in conditions))\n",
    "        patient_features['history_cellulitis'] = int(any('cellulitis' in c for c in conditions))\n",
    "        patient_features['history_uti'] = int(any('urinary tract infection' in c for c in conditions))\n",
    "        patient_features['history_seizure'] = int(any(s in c for s in ['epilepsy', 'seizure'] for c in conditions))\n",
    "        patient_features['history_dementia'] = int(any(s in c for s in ['dementia', \"alzheimer's\"] for c in conditions))\n",
    "        patient_features['history_anxiety'] = int(any('anxiety' in c for c in conditions))\n",
    "        patient_features['history_depression'] = int(any('depression' in c for c in conditions))\n",
    "\n",
    "        patient_features['on_anticoagulant'] = int(any(s in m for s in ['warfarin', 'eliquis', 'xarelto', 'heparin'] for m in medications))\n",
    "        patient_features['on_beta_blocker'] = int(any(s in m for s in ['metoprolol', 'carvedilol', 'atenolol'] for m in medications))\n",
    "        patient_features['on_diuretic'] = int(any(s in m for s in ['furosemide', 'hydrochlorothiazide', 'spironolactone'] for m in medications))\n",
    "        patient_features['on_ccb'] = int(any(s in m for s in ['amlodipine', 'diltiazem'] for m in medications))\n",
    "        patient_features['on_insulin'] = int(any('insulin' in m for m in medications))\n",
    "        patient_features['on_ppi'] = int(any(s in m for s in ['omeprazole', 'pantoprazole', 'esomeprazole'] for m in medications))\n",
    "        patient_features['on_thyroid_med'] = int(any('levothyroxine' in m for m in medications))\n",
    "        patient_features['on_gout_med'] = int(any('allopurinol' in m for m in medications))\n",
    "        patient_features['proc_colonoscopy'] = int(any('colonoscopy' in p for p in procedures))\n",
    "        patient_features['proc_egd'] = int(any(s in p for s in ['egd', 'esophagogastroduodenoscopy'] for p in procedures))\n",
    "        patient_features['proc_dialysis'] = int(any('dialysis' in p for p in procedures))\n",
    "\n",
    "        # --- NEWLY ADDED 14 HISTORY FLAGS ---\n",
    "        patient_features['history_gerd'] = int(any('gerd' in c for c in conditions))\n",
    "        patient_features['history_allergies'] = int(any('allergies' in c for c in conditions))\n",
    "        patient_features['history_migraine'] = int(any('migraine' in c for c in conditions))\n",
    "        patient_features['history_sleep_apnea'] = int(any('sleep apnea' in c for c in conditions))\n",
    "        patient_features['history_low_back_pain'] = int(any('low back pain' in c for c in conditions))\n",
    "        patient_features['history_smoker'] = int(any('nicotine dependence' in c for c in conditions))\n",
    "        patient_features['history_alcohol_use'] = int(any('alcohol use disorder' in c for c in conditions))\n",
    "        patient_features['history_eczema'] = int(any('atopic dermatitis' in c for c in conditions))\n",
    "        patient_features['history_iron_deficiency'] = int(any('iron deficiency' in c for c in conditions))\n",
    "        patient_features['history_osteoporosis'] = int(any('osteoporosis' in c for c in conditions))\n",
    "        patient_features['history_insomnia'] = int(any('insomnia' in c for c in conditions))\n",
    "        patient_features['history_ibs'] = int(any('ibs' in c for c in conditions))\n",
    "        patient_features['history_sinusitis'] = int(any('sinusitis' in c for c in conditions))\n",
    "        patient_features['history_dizziness'] = int(any(s in c for s in ['dizziness', 'vertigo'] for c in conditions))\n",
    "        # --- END OF MODIFICATIONS ---\n",
    "\n",
    "        obs_map = {\n",
    "            '8480-6':'sbp', '8462-4':'dbp', '8867-4':'heart_rate', '9279-1':'respiratory_rate', '59408-5':'spo2',\n",
    "            '8310-5':'temperature', '6690-2':'wbc_count', '718-7':'hemoglobin', '4544-3':'hematocrit', '777-3':'platelet_count',\n",
    "            '2093-3':'cholesterol_total', '2571-8':'triglycerides', '18262-6':'cholesterol_ldl', '2085-9':'cholesterol_hdl',\n",
    "            '72514-3':'pain_score', '48643-1':'hba1c', '55758-7':'phq2_score', '70274-6':'gad7_score', '8302-2':'height_m',\n",
    "            '29463-7':'weight_kg', '3094-0':'bun', '2951-2':'sodium', '2823-3':'potassium', '2345-7':'glucose',\n",
    "            '1975-2':'bilirubin', '2160-0':'creatinine', '10839-9': 'troponin_i', '1988-5': 'crp', '2524-7': 'lactate',\n",
    "            '33959-8': 'procalcitonin', '48065-7': 'd_dimer', '5821-4': 'urine_wbc', '5811-5': 'urine_nitrite',\n",
    "            '5799-2': 'urine_leukocyte', '14563-1': 'stool_occult_blood', '1751-7': 'albumin', '1742-6': 'alt',\n",
    "            '1920-8': 'ast', '2991-8': 'tsh', '3005-4': 'free_t4', '14338-8': 'lipase', '1798-8': 'amylase',\n",
    "            '34714-6': 'inr', '3173-2': 'ptt', '2601-5': 'magnesium', '2777-1': 'phosphate', '11568-8': 'rheumatoid_factor'\n",
    "        }\n",
    "\n",
    "        patient_rows = []\n",
    "        for obs_time, obs in observations:\n",
    "            row = patient_features.copy()\n",
    "            row['obs_time'] = obs_time\n",
    "            if 'component' in obs:\n",
    "                for comp in obs['component']:\n",
    "                    code = comp.get('code', {}).get('coding', [{}])[0].get('code')\n",
    "                    if code in obs_map: row[obs_map[code]] = comp.get('valueQuantity', {}).get('value')\n",
    "            else:\n",
    "                code = obs.get('code', {}).get('coding', [{}])[0].get('code')\n",
    "                if code in obs_map: row[obs_map[code]] = obs.get('valueQuantity', {}).get('value')\n",
    "            patient_rows.append(row)\n",
    "        return patient_rows\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def process_and_save(file_path, output_dir):\n",
    "    patient_data = parse_fhir_bundle_24h(file_path) # Uses the new efficient/robust version\n",
    "    if patient_data:\n",
    "        patient_id = os.path.basename(file_path).split('.')[0]\n",
    "        pd.DataFrame(patient_data).to_parquet(\n",
    "            os.path.join(output_dir, f'{patient_id}.parquet'),\n",
    "            compression='snappy', index=False\n",
    "        )\n",
    "\n",
    "# Setup directories\n",
    "data_directory = '/content/fhir/'\n",
    "temp_output_dir = '/content/temp_parquet/'\n",
    "if os.path.exists(temp_output_dir):\n",
    "    shutil.rmtree(temp_output_dir)\n",
    "os.makedirs(temp_output_dir)\n",
    "\n",
    "all_json_files = [os.path.join(data_directory, f) for f in os.listdir(data_directory) if f.endswith('.json')]\n",
    "print(f\"Total candidate JSON files: {len(all_json_files)}\")\n",
    "\n",
    "print(\"\\n--- Parsing in parallel and saving to temporary files... ---\")\n",
    "# Limit workers to 2 to conserve RAM in the free Colab environment\n",
    "with ProcessPoolExecutor(max_workers=2) as executor:\n",
    "    futures = {executor.submit(process_and_save, fp, temp_output_dir): fp for fp in all_json_files}\n",
    "    for i, future in enumerate(as_completed(futures), 1):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processed {i} of {len(all_json_files)} files...\")\n",
    "            gc.collect()\n",
    "\n",
    "print(\"\\n✅ All files parsed and saved to disk.\")\n",
    "\n",
    "# Delete source files immediately to free disk space\n",
    "print(\"--- Removing source JSON files to free disk space... ---\")\n",
    "if os.path.exists(data_directory):\n",
    "    shutil.rmtree(data_directory)\n",
    "gc.collect()\n",
    "\n",
    "print(\"--- Source JSON directory removed. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7072938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"Aggressively reduce memory footprint\"\"\"\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        if df[col].min() >= 0 and df[col].max() <= 1: df[col] = df[col].astype('int8')\n",
    "        elif df[col].min() >= -128 and df[col].max() <= 127: df[col] = df[col].astype('int8')\n",
    "        elif df[col].min() >= -32768 and df[col].max() <= 32767: df[col] = df[col].astype('int16')\n",
    "        else: df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "print(\"--- Processing data with streaming writes (NO RAM SPIKE!) ---\")\n",
    "temp_output_dir = '/content/temp_parquet/'\n",
    "all_parquet_files = [os.path.join(temp_output_dir, f) for f in os.listdir(temp_output_dir) if f.endswith('.parquet')]\n",
    "\n",
    "final_chunks_dir = '/content/final_parquet_chunks/'\n",
    "if os.path.exists(final_chunks_dir):\n",
    "    shutil.rmtree(final_chunks_dir)\n",
    "os.makedirs(final_chunks_dir)\n",
    "\n",
    "chunk_size = 2000\n",
    "\n",
    "essential_columns = [\n",
    "    'sbp', 'dbp', 'heart_rate', 'respiratory_rate', 'wbc_count', 'hemoglobin', 'hematocrit', 'platelet_count',\n",
    "    'cholesterol_total', 'triglycerides', 'cholesterol_ldl', 'cholesterol_hdl', 'hba1c', 'pain_score', 'phq2_score',\n",
    "    'gad7_score', 'height_m', 'weight_kg', 'bun', 'sodium', 'potassium', 'glucose', 'bilirubin', 'temperature',\n",
    "    'creatinine', 'spo2', 'troponin_i', 'crp', 'lactate', 'procalcitonin', 'd_dimer', 'urine_wbc', 'urine_nitrite',\n",
    "    'urine_leukocyte', 'stool_occult_blood', 'albumin', 'alt', 'ast', 'tsh', 'free_t4', 'lipase', 'amylase',\n",
    "    'inr', 'ptt', 'magnesium', 'phosphate', 'rheumatoid_factor'\n",
    "]\n",
    "\n",
    "# --- 1. FIX: List ALL columns used in rules to pre-populate them ---\n",
    "all_rule_cols = [\n",
    "    # Labs\n",
    "    'lactate', 'crp', 'troponin_i', 'phq2_score', 'gad7_score', 'urine_wbc', 'urine_nitrite', 'urine_leukocyte', 'd_dimer',\n",
    "    'stool_occult_blood', 'lipase', 'amylase', 'inr', 'tsh', 'uric_acid', 'rheumatoid_factor', 'albumin', 'magnesium',\n",
    "    'phosphate', 'chloride', 'bicarbonate', 'alt', 'ast', 'temperature', 'pain_score', 'hba1c', 'bilirubin', 'potassium',\n",
    "    'platelet_count', 'sodium', 'wbc_count', 'age',\n",
    "\n",
    "    # History/Med Flags\n",
    "    'history_cad', 'history_afib', 'history_hypertension', 'history_pneumonia', 'history_chf', 'history_diabetes',\n",
    "    'history_ckd', 'history_copd', 'on_statin', 'history_uti', 'history_dvt_pe', 'history_gi_bleed',\n",
    "    'history_pancreatitis', 'on_anticoagulant', 'history_thyroid', 'history_gout', 'history_arthritis',\n",
    "    'history_ibd', 'history_cellulitis', 'history_seizure', 'history_dementia', 'on_lisinopril', 'on_aspirin',\n",
    "    'on_beta_blocker', 'on_diuretic', 'history_obesity', 'history_anxiety', 'history_depression', 'on_ppi',\n",
    "\n",
    "    # --- NEWLY ADDED 14 FLAGS ---\n",
    "    'history_gerd', 'history_allergies', 'history_migraine', 'history_sleep_apnea',\n",
    "    'history_low_back_pain', 'history_smoker', 'history_alcohol_use', 'history_eczema',\n",
    "    'history_iron_deficiency', 'history_osteoporosis', 'history_insomnia',\n",
    "    'history_ibs', 'history_sinusitis', 'history_dizziness',\n",
    "    # --- END OF MODIFICATIONS ---\n",
    "\n",
    "    # Delta Flags\n",
    "    'delta_heart_rate', 'delta_hemoglobin', 'delta_creatinine'\n",
    "]\n",
    "\n",
    "\n",
    "total_chunks = (len(all_parquet_files) - 1) // chunk_size + 1\n",
    "\n",
    "for i in range(0, len(all_parquet_files), chunk_size):\n",
    "    chunk_files = all_parquet_files[i:i + chunk_size]\n",
    "    if not chunk_files: continue\n",
    "    current_chunk_num = i // chunk_size + 1\n",
    "    print(f\"Processing chunk {current_chunk_num}/{total_chunks}...\")\n",
    "\n",
    "    df_chunk = pd.concat([pd.read_parquet(f) for f in chunk_files], ignore_index=True)\n",
    "\n",
    "    for col in essential_columns:\n",
    "        if col not in df_chunk.columns: df_chunk[col] = np.nan\n",
    "\n",
    "    df_chunk['obs_time'] = pd.to_datetime(df_chunk['obs_time'], utc=True)\n",
    "    df_chunk = df_chunk.sort_values(['patient_id', 'obs_time']).set_index('obs_time')\n",
    "\n",
    "    vitals_to_track = [\n",
    "        'heart_rate', 'sbp', 'dbp', 'respiratory_rate', 'temperature', 'glucose',\n",
    "        'wbc_count', 'hemoglobin', 'creatinine', 'potassium', 'lactate', 'troponin_i'\n",
    "    ]\n",
    "    for vital in vitals_to_track:\n",
    "        if vital in df_chunk.columns:\n",
    "            rolling_window = df_chunk.groupby('patient_id')[vital].rolling(window='3h', min_periods=1)\n",
    "            df_chunk[f'{vital}_avg_3hr'] = rolling_window.mean().reset_index(0, drop=True)\n",
    "            df_chunk[f'{vital}_std_3hr'] = rolling_window.std().reset_index(0, drop=True)\n",
    "\n",
    "    df_chunk.reset_index(inplace=True)\n",
    "    cols_to_fill = df_chunk.columns.difference(['patient_id'])\n",
    "    df_chunk[cols_to_fill] = df_chunk.groupby('patient_id')[cols_to_fill].ffill()\n",
    "    df_chunk.fillna(0, inplace=True)\n",
    "\n",
    "    if 'gender' in df_chunk.columns:\n",
    "        df_chunk = pd.get_dummies(df_chunk, columns=['gender'], drop_first=True, dtype='int8')\n",
    "\n",
    "    safe_height_sq = (df_chunk['height_m'].replace(0, np.nan)) ** 2\n",
    "    safe_sbp = df_chunk['sbp'].replace(0, np.nan)\n",
    "    safe_creatinine = df_chunk['creatinine'].replace(0, np.nan)\n",
    "    safe_hemoglobin = df_chunk['hemoglobin'].replace(0, np.nan)\n",
    "\n",
    "    df_chunk['map'] = ((2 * df_chunk['dbp']) + df_chunk['sbp']) / 3\n",
    "    df_chunk['shock_index'] = df_chunk['heart_rate'] / safe_sbp\n",
    "    df_chunk['bun_creatinine_ratio'] = df_chunk['bun'] / safe_creatinine\n",
    "\n",
    "    # Ensure all rule columns exist before calculating rules\n",
    "    for col in all_rule_cols:\n",
    "         if col not in df_chunk.columns:\n",
    "            df_chunk[col] = 0\n",
    "\n",
    "    df_chunk['anion_gap'] = df_chunk['sodium'] - (df_chunk['chloride'] + df_chunk['bicarbonate'])\n",
    "    df_chunk['pulse_pressure'] = df_chunk['sbp'] - df_chunk['dbp']\n",
    "\n",
    "    delta_cols_list = ['sbp', 'dbp', 'heart_rate', 'wbc_count', 'hemoglobin', 'map', 'creatinine', 'lactate', 'troponin_i']\n",
    "    for col in delta_cols_list:\n",
    "        if col in df_chunk.columns:\n",
    "            df_chunk[f'delta_{col}'] = df_chunk.groupby('patient_id')[col].diff().fillna(0)\n",
    "\n",
    "    # --- MULTI-TIMEFRAME LABEL ENGINEERING (NOW 65 CONDITIONS) ---\n",
    "    conditions = {\n",
    "        # --- Original 28 Conditions ---\n",
    "        'sepsis': [((df_chunk['heart_rate'] > 110) & (df_chunk['wbc_count'] > 14) & (df_chunk['lactate'] > 2)), ((df_chunk['heart_rate'] > 100) & (df_chunk['wbc_count'] > 12)), ((df_chunk['heart_rate'] > 90) & (df_chunk['wbc_count'] > 11))],\n",
    "        'anemia': [(df_chunk['hemoglobin'] < 10), (df_chunk['hemoglobin'] < 12), (df_chunk['hemoglobin'] < 13)],\n",
    "        'hyperlipidemia': [((df_chunk['cholesterol_total'] > 240) | (df_chunk['cholesterol_ldl'] > 160)), ((df_chunk['cholesterol_total'] > 200) | (df_chunk['cholesterol_ldl'] > 130)), (df_chunk['on_statin'] == 1)],\n",
    "        'mi': [(df_chunk['troponin_i'] > 0.1), (df_chunk['troponin_i'] > 0.04), ((df_chunk['history_cad'] == 1) & (df_chunk['pain_score'] > 5))],\n",
    "        'stroke': [((df_chunk['history_afib'] == 1) & (df_chunk['age'] > 70)), ((df_chunk['history_hypertension'] == 1) & (df_chunk['age'] > 60)), (df_chunk['age'] > 55)],\n",
    "        'depression': [(df_chunk['phq2_score'] > 4), (df_chunk['phq2_score'] > 2), (df_chunk['history_depression'] == 1)],\n",
    "        'anxiety': [(df_chunk['gad7_score'] > 15), (df_chunk['gad7_score'] > 9), (df_chunk['history_anxiety'] == 1)],\n",
    "        'pneumonia': [((df_chunk['respiratory_rate'] > 24) & (df_chunk['wbc_count'] > 13) & (df_chunk['crp'] > 50)), ((df_chunk['respiratory_rate'] > 20) & (df_chunk['wbc_count'] > 12)), (df_chunk['history_pneumonia'] == 1)],\n",
    "        'chf_exacerbation': [((df_chunk['history_chf'] == 1) & (df_chunk['respiratory_rate'] > 22)), ((df_chunk['history_chf'] == 1) & (df_chunk['respiratory_rate'] > 20)), (df_chunk['history_chf'] == 1)],\n",
    "        'hypertension': [((df_chunk['sbp'] > 160) | (df_chunk['dbp'] > 100)), ((df_chunk['sbp'] > 140) | (df_chunk['dbp'] > 90)), (df_chunk['history_hypertension'] == 1)],\n",
    "        'diabetes': [(df_chunk['glucose'] > 200), ((df_chunk['hba1c'] > 6.5) | (df_chunk['glucose'] > 126)), (df_chunk['history_diabetes'] == 1)],\n",
    "        'hypoglycemia': [(df_chunk['glucose'] < 60), (df_chunk['glucose'] < 70), (df_chunk['glucose'] < 80)],\n",
    "        'aki': [(df_chunk['creatinine'] > (df_chunk['creatinine'].median() + 0.5)), (df_chunk['creatinine'] > (df_chunk['creatinine'].median() + 0.3)), (df_chunk['creatinine'] > (df_chunk['creatinine'].median() + 0.2))],\n",
    "        'tachycardia': [(df_chunk['heart_rate'] > 120), (df_chunk['heart_rate'] > 100), (df_chunk['heart_rate'] > 95)],\n",
    "        'bradycardia': [(df_chunk['heart_rate'] < 50), (df_chunk['heart_rate'] < 60), (df_chunk['heart_rate'] < 65)],\n",
    "        'hypotension': [(df_chunk['sbp'] < 85), (df_chunk['sbp'] < 90), (df_chunk['sbp'] < 95)],\n",
    "        'acute_bronchitis': [((df_chunk['respiratory_rate'] > 22) & (df_chunk['wbc_count'] < 10)), ((df_chunk['respiratory_rate'] > 20) & (df_chunk['wbc_count'] < 12)), ((df_chunk['respiratory_rate'] > 18) & (df_chunk['wbc_count'] < 12))],\n",
    "        'ckd': [(df_chunk['creatinine'] > 2.0), (df_chunk['creatinine'] > 1.5), (df_chunk['history_ckd'] == 1)],\n",
    "        'copd_exacerbation': [((df_chunk['history_copd'] == 1) & (df_chunk['respiratory_rate'] > 22)), ((df_chunk['history_copd'] == 1) & (df_chunk['respiratory_rate'] > 20)), (df_chunk['history_copd'] == 1)],\n",
    "        'liver_disease': [(df_chunk['bilirubin'] > 2.5), ((df_chunk['alt'] > 100) | (df_chunk['ast'] > 100)), (df_chunk['bilirubin'] > 1.8)],\n",
    "        'hypokalemia': [(df_chunk['potassium'] < 3.0), (df_chunk['potassium'] < 3.5), (df_chunk['potassium'] < 3.7)],\n",
    "        'hypernatremia': [(df_chunk['sodium'] > 150), (df_chunk['sodium'] > 145), (df_chunk['sodium'] > 142)],\n",
    "        'obesity': [((df_chunk['weight_kg'] / safe_height_sq) > 35), ((df_chunk['weight_kg'] / safe_height_sq) > 30), (df_chunk['history_obesity'] == 1)],\n",
    "        'dehydration': [(df_chunk['bun_creatinine_ratio'] > 25), (df_chunk['bun_creatinine_ratio'] > 20), (df_chunk['sodium'] > 145)],\n",
    "        'thrombocytopenia': [(df_chunk['platelet_count'] < 100), (df_chunk['platelet_count'] < 150), (df_chunk['platelet_count'] < 180)],\n",
    "        'hyperkalemia': [(df_chunk['potassium'] > 5.5), (df_chunk['potassium'] > 5.0), (df_chunk['potassium'] > 4.8)],\n",
    "        'hyponatremia': [(df_chunk['sodium'] < 130), (df_chunk['sodium'] < 135), (df_chunk['sodium'] < 137)],\n",
    "        'leukopenia': [(df_chunk['wbc_count'] < 3.0), (df_chunk['wbc_count'] < 4.0), (df_chunk['wbc_count'] < 4.5)],\n",
    "\n",
    "        # --- Original 23 New Conditions ---\n",
    "        'uti_risk': [((df_chunk['urine_wbc'] > 50) & (df_chunk['urine_nitrite'] == 1)), ((df_chunk['urine_wbc'] > 10) | (df_chunk['urine_leukocyte'] == 1)), (df_chunk['history_uti'] == 1)],\n",
    "        'pulmonary_embolism': [((df_chunk['d_dimer'] > 500) & (df_chunk['heart_rate'] > 100)), (df_chunk['d_dimer'] > 250), (df_chunk['history_dvt_pe'] == 1)],\n",
    "        'atrial_fibrillation': [((df_chunk['heart_rate'] > 120) & (df_chunk['delta_heart_rate'].abs() > 30)), ((df_chunk['heart_rate'] > 100) & (df_chunk['age'] > 65)), (df_chunk['history_afib'] == 1)],\n",
    "        'gi_bleed': [(df_chunk['delta_hemoglobin'] < -2.0), (df_chunk['stool_occult_blood'] == 1), (df_chunk['history_gi_bleed'] == 1)],\n",
    "        'pancreatitis': [((df_chunk['lipase'] > 300) | (df_chunk['amylase'] > 300)), ((df_chunk['lipase'] > 150) | (df_chunk['amylase'] > 150)), (df_chunk['history_pancreatitis'] == 1)],\n",
    "        'coagulopathy': [(df_chunk['inr'] > 3.0), (df_chunk['inr'] > 1.5), (df_chunk['on_anticoagulant'] == 1)],\n",
    "        'dvt_risk': [(df_chunk['d_dimer'] > 500), (df_chunk['d_dimer'] > 250), (df_chunk['history_dvt_pe'] == 1)],\n",
    "        'hypothyroidism': [(df_chunk['tsh'] > 10.0), (df_chunk['tsh'] > 4.5), (df_chunk['history_thyroid'] == 1)],\n",
    "        'hyperthyroidism': [(df_chunk['tsh'] < 0.1), (df_chunk['tsh'] < 0.4), ((df_chunk['history_thyroid'] == 1) & (df_chunk['heart_rate'] > 100))],\n",
    "        'gout': [(df_chunk['uric_acid'] > 9.0), (df_chunk['uric_acid'] > 7.0), (df_chunk['history_gout'] == 1)],\n",
    "        'arthritis': [(df_chunk['rheumatoid_factor'] > 20), (df_chunk['crp'] > 10), (df_chunk['history_arthritis'] == 1)],\n",
    "        'ibd_exacerbation': [((df_chunk['history_ibd'] == 1) & (df_chunk['crp'] > 20)), ((df_chunk['history_ibd'] == 1) & (df_chunk['hemoglobin'] < 10)), (df_chunk['history_ibd'] == 1)],\n",
    "        'cellulitis': [((df_chunk['history_cellulitis'] == 1) & (df_chunk['wbc_count'] > 12)), ((df_chunk['history_cellulitis'] == 1) & (df_chunk['temperature'] > 38.0)), (df_chunk['history_cellulitis'] == 1)],\n",
    "        'seizure_risk': [(df_chunk['history_seizure'] == 1), (df_chunk['history_seizure'] == 1), (df_chunk['history_seizure'] == 1)],\n",
    "        'dementia_risk': [(df_chunk['history_dementia'] == 1), (df_chunk['history_dementia'] == 1), (df_chunk['history_dementia'] == 1)],\n",
    "        'malnutrition': [(df_chunk['albumin'] < 2.5), (df_chunk['albumin'] < 3.4), ((df_chunk['age'] > 75) & (df_chunk['albumin'] < 3.5))],\n",
    "        'drug_side_effect_renal': [((df_chunk['on_lisinopril'] == 1) & (df_chunk['delta_creatinine'] > 0.3)), ((df_chunk['on_lisinopril'] == 1) & (df_chunk['potassium'] > 5.0)), (df_chunk['on_lisinopril'] == 1)],\n",
    "        'drug_side_effect_bleed': [(((df_chunk['on_aspirin'] == 1) | (df_chunk['on_anticoagulant'] == 1)) & (df_chunk['delta_hemoglobin'] < -1.0)), (((df_chunk['on_aspirin'] == 1) | (df_chunk['on_anticoagulant'] == 1)) & (df_chunk['platelet_count'] < 150)), ((df_chunk['on_aspirin'] == 1) | (df_chunk['on_anticoagulant'] == 1))],\n",
    "        'drug_side_effect_bradycardia': [((df_chunk['on_beta_blocker'] == 1) & (df_chunk['heart_rate'] < 50)), ((df_chunk['on_beta_blocker'] == 1) & (df_chunk['heart_rate'] < 60)), (df_chunk['on_beta_blocker'] == 1)],\n",
    "        'hypomagnesemia': [(df_chunk['magnesium'] < 1.2), (df_chunk['magnesium'] < 1.8), (df_chunk['on_diuretic'] == 1)],\n",
    "        'hypophosphatemia': [(df_chunk['phosphate'] < 2.0), (df_chunk['phosphate'] < 2.5), (df_chunk['history_diabetes'] == 1)],\n",
    "        'acidosis': [(df_chunk['anion_gap'] > 20), (df_chunk['anion_gap'] > 16), (df_chunk['lactate'] > 2.0)],\n",
    "        'rhabdomyolysis': [((df_chunk['creatinine'] > 2.0) & (df_chunk['potassium'] > 5.5)), (df_chunk['on_statin'] == 1), (df_chunk['on_statin'] == 1)],\n",
    "\n",
    "        # --- NEW 14 BASIC CONDITIONS ---\n",
    "        'gerd_risk': [(df_chunk['on_ppi'] == 1), (df_chunk['history_gerd'] == 1), (df_chunk['history_gerd'] == 1)],\n",
    "        'allergy_risk': [(df_chunk['history_allergies'] == 1), (df_chunk['history_allergies'] == 1), (df_chunk['history_allergies'] == 1)],\n",
    "        'migraine_risk': [(df_chunk['history_migraine'] == 1), (df_chunk['history_migraine'] == 1), (df_chunk['history_migraine'] == 1)],\n",
    "        'sleep_apnea_risk': [((df_chunk['history_sleep_apnea'] == 1) & (df_chunk['history_obesity'] == 1)), (df_chunk['history_sleep_apnea'] == 1), (df_chunk['history_obesity'] == 1)],\n",
    "        'low_back_pain_risk': [(df_chunk['history_low_back_pain'] == 1), (df_chunk['history_low_back_pain'] == 1), (df_chunk['history_low_back_pain'] == 1)],\n",
    "        'smoker_risk': [(df_chunk['history_smoker'] == 1), (df_chunk['history_smoker'] == 1), (df_chunk['history_smoker'] == 1)],\n",
    "        'alcohol_use_risk': [((df_chunk['history_alcohol_use'] == 1) & (df_chunk['alt'] > 50)), (df_chunk['history_alcohol_use'] == 1), (df_chunk['history_alcohol_use'] == 1)],\n",
    "        'eczema_risk': [(df_chunk['history_eczema'] == 1), (df_chunk['history_eczema'] == 1), (df_chunk['history_eczema'] == 1)],\n",
    "        'iron_deficiency_risk': [(df_chunk['history_iron_deficiency'] == 1), (df_chunk['history_iron_deficiency'] == 1), (df_chunk['history_anemia'] == 1)],\n",
    "        'osteoporosis_risk': [(df_chunk['history_osteoporosis'] == 1), (df_chunk['history_osteoporosis'] == 1), (df_chunk['age'] > 65)],\n",
    "        'insomnia_risk': [(df_chunk['history_insomnia'] == 1), (df_chunk['history_insomnia'] == 1), (df_chunk['history_anxiety'] == 1)],\n",
    "        'ibs_risk': [(df_chunk['history_ibs'] == 1), (df_chunk['history_ibs'] == 1), (df_chunk['history_ibs'] == 1)],\n",
    "        'sinusitis_risk': [(df_chunk['history_sinusitis'] == 1), (df_chunk['history_sinusitis'] == 1), (df_chunk['history_allergies'] == 1)],\n",
    "        'dizziness_risk': [(df_chunk['history_dizziness'] == 1), (df_chunk['history_dizziness'] == 1), (df_chunk['history_dizziness'] == 1)]\n",
    "    }\n",
    "\n",
    "    # --- 2. FIX: Address PerformanceWarning by creating columns in a dict first ---\n",
    "    new_risk_cols = {}\n",
    "    timeframes = ['_6h', '_24h', '_48h']\n",
    "    for name, rules in conditions.items():\n",
    "        for idx, time in enumerate(timeframes):\n",
    "            new_risk_cols[f'risk_{name}{time}'] = rules[idx].astype('int8')\n",
    "\n",
    "    df_chunk = pd.concat([df_chunk, pd.DataFrame(new_risk_cols, index=df_chunk.index)], axis=1)\n",
    "    # --- End of Fix 2 ---\n",
    "\n",
    "    df_chunk.fillna(0, inplace=True)\n",
    "    df_chunk = optimize_dtypes(df_chunk)\n",
    "\n",
    "    output_filename = os.path.join(final_chunks_dir, f\"processed_chunk_{current_chunk_num}.parquet\")\n",
    "    df_chunk.to_parquet(output_filename, engine='pyarrow', compression='snappy', index=False)\n",
    "\n",
    "    print(f\"    -> Chunk processed and written to {output_filename}. RAM freed.\")\n",
    "    del df_chunk, new_risk_cols\n",
    "    gc.collect()\n",
    "\n",
    "# Clean up temp directory\n",
    "print(\"\\n--- Removing temporary parquet files... ---\")\n",
    "if os.path.exists(temp_output_dir):\n",
    "    shutil.rmtree(temp_output_dir)\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n✅ All chunks processed and saved to '{final_chunks_dir}'\")\n",
    "print(f\"Total chunks saved: {total_chunks}. Total conditions: {len(conditions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f170d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "import math\n",
    "import pyarrow.parquet as pq # Needed to read file metadata\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. Get File Lists and Column Names ---\n",
    "final_chunks_dir = '/content/final_parquet_chunks/'\n",
    "all_chunk_files = [os.path.join(final_chunks_dir, f) for f in os.listdir(final_chunks_dir) if f.endswith('.parquet')]\n",
    "all_chunk_files.sort()\n",
    "\n",
    "print(\"--- Reading column names from first chunk... ---\")\n",
    "if not all_chunk_files:\n",
    "    print(\"❌ ERROR: No .parquet files found in /content/final_parquet_chunks/\")\n",
    "    raise FileNotFoundError(\"No processed chunk files found. Please run the previous script.\")\n",
    "\n",
    "sample_df = pd.read_parquet(all_chunk_files[0])\n",
    "target_diseases_multitime = [col for col in sample_df.columns if col.startswith('risk_')]\n",
    "feature_columns = [col for col in sample_df.columns if col not in target_diseases_multitime and col not in ['patient_id', 'obs_time', 'obs_time_utc']]\n",
    "base_diseases = sorted(list(set([c.replace('risk_','').split('_')[0] for c in target_diseases_multitime])))\n",
    "del sample_df\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Found {len(feature_columns)} features and {len(target_diseases_multitime)} targets.\")\n",
    "\n",
    "# --- 2. Fit the Scaler (Memory-Safe) ---\n",
    "print(\"--- Fitting StandardScaler in chunks (memory-safe)... ---\")\n",
    "scaler = StandardScaler()\n",
    "total_rows = 0\n",
    "\n",
    "for f in all_chunk_files:\n",
    "    df_chunk = pd.read_parquet(f, columns=feature_columns)\n",
    "    scaler.partial_fit(df_chunk)\n",
    "    total_rows += len(df_chunk)\n",
    "    del df_chunk\n",
    "\n",
    "print(f\"Scaler fit on {total_rows} total rows.\")\n",
    "print(\"--- Saving scaler... ---\")\n",
    "joblib.dump(scaler, 'data_scaler.joblib')\n",
    "\n",
    "# --- 3. Split files for training and validation ---\n",
    "if len(all_chunk_files) > 1:\n",
    "    print(f\"Splitting {len(all_chunk_files)} chunks into train/val sets...\")\n",
    "    train_files, val_files = train_test_split(all_chunk_files, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    print(f\"⚠️ Warning: Only {len(all_chunk_files)} data chunk found. Using it for BOTH training and validation to test the pipeline.\")\n",
    "    train_files = all_chunk_files\n",
    "    val_files = all_chunk_files\n",
    "\n",
    "print(f\"Using {len(train_files)} file(s) for training, {len(val_files)} file(s) for validation.\")\n",
    "\n",
    "\n",
    "# --- 4. Define the Keras Data Generator (FIX #1: Solves System RAM OOM) ---\n",
    "def data_generator(file_list, scaler, feature_cols, target_cols, batch_size):\n",
    "    while True:\n",
    "        shuffled_files = np.random.permutation(file_list)\n",
    "\n",
    "        for f in shuffled_files:\n",
    "            try:\n",
    "                parquet_file = pq.ParquetFile(f)\n",
    "                for batch in parquet_file.iter_batches(batch_size=batch_size):\n",
    "                    batch_df = batch.to_pandas()\n",
    "                    batch_df = batch_df.sample(frac=1)\n",
    "                    X_pre_scale = batch_df.reindex(columns=feature_cols).fillna(0)\n",
    "                    y = batch_df.reindex(columns=target_cols).fillna(0)\n",
    "                    X_scaled = scaler.transform(X_pre_scale)\n",
    "                    y_dict = {col: y[col] for col in y.columns}\n",
    "                    yield X_scaled, y_dict\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error reading or streaming {f}: {e}. Skipping file.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "# --- 5. Define the Multi-Task Neural Network (FIX #2: LIGHTEST GPU VRAM Model) ---\n",
    "def create_multi_task_model(n_features, targets):\n",
    "    input_layer = Input(shape=(n_features,), name='input_features')\n",
    "    x = BatchNormalization()(input_layer)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Shared \"Body\" - This is now even smaller (64 -> 32) to save VRAM\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    shared_body = Dense(32, activation='relu')(x) # Was 64\n",
    "\n",
    "    output_layers = []\n",
    "    output_losses = {}\n",
    "    output_metrics = {}\n",
    "\n",
    "    for target_name in targets:\n",
    "        # Simplified heads (no extra Dense(16) layer)\n",
    "        output = Dense(1, activation='sigmoid', name=target_name)(shared_body)\n",
    "        output_layers.append(output)\n",
    "        output_losses[target_name] = 'binary_crossentropy'\n",
    "        output_metrics[target_name] = tf.keras.metrics.AUC(name=f'{target_name}_auc')\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layers)\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=output_losses,\n",
    "                  metrics=output_metrics)\n",
    "    return model\n",
    "\n",
    "model = create_multi_task_model(len(feature_columns), target_diseases_multitime)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# --- 6. Train the Model using the Generator (MODIFIED FOR 1 EPOCH) ---\n",
    "print(\"\\n--- Training Multi-Task Neural Network from disk... ---\")\n",
    "\n",
    "# Using the Batch Size that worked in your previous log\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_rows = sum([pq.ParquetFile(f).metadata.num_rows for f in train_files])\n",
    "val_rows = sum([pq.ParquetFile(f).metadata.num_rows for f in val_files])\n",
    "\n",
    "steps_per_epoch = math.ceil(train_rows / BATCH_SIZE)\n",
    "validation_steps = math.ceil(val_rows / BATCH_SIZE)\n",
    "\n",
    "print(f\"Train Rows: {train_rows}, Val Rows: {val_rows}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per Epoch: {steps_per_epoch}, Validation Steps: {validation_steps}\")\n",
    "\n",
    "# Define the data types and shapes for the tf.data.Dataset\n",
    "n_features = len(feature_columns)\n",
    "X_spec = tf.TensorSpec(shape=(None, n_features), dtype=tf.float32)\n",
    "y_spec = {\n",
    "    target_name: tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    "    for target_name in target_diseases_multitime\n",
    "}\n",
    "output_signature = (X_spec, y_spec)\n",
    "\n",
    "# Create the Dataset objects\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_files, scaler, feature_columns, target_diseases_multitime, BATCH_SIZE),\n",
    "    output_signature=output_signature\n",
    ")\n",
    "val_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_files, scaler, feature_columns, target_diseases_multitime, BATCH_SIZE),\n",
    "    output_signature=output_signature\n",
    ")\n",
    "\n",
    "# This is the fix for RAM paging (the slowdown)\n",
    "train_ds = train_ds.prefetch(5)\n",
    "val_ds = val_ds.prefetch(5)\n",
    "\n",
    "print(\"\\nStarting model.fit() for exactly 1 epoch...\")\n",
    "\n",
    "# --- START OF 1-EPOCH MODIFICATION ---\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=1,  # Set to 1\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[], # No callbacks needed for 1 epoch\n",
    "    verbose=1\n",
    ")\n",
    "# --- END OF 1-EPOCH MODIFICATION ---\n",
    "\n",
    "print(\"✅ Multi-Task model training complete!\\n\")\n",
    "\n",
    "# --- 7. Save Artifacts for GUI and Deployment ---\n",
    "print(\"\\n--- Saving model and helper files... ---\")\n",
    "model.save('patient_risk_model.keras')\n",
    "# Scaler was already saved in step 2\n",
    "\n",
    "with open('feature_columns_multitime.json', 'w') as f:\n",
    "    json.dump(feature_columns, f)\n",
    "with open('target_diseases_base.json', 'w') as f:\n",
    "    json.dump(base_diseases, f)\n",
    "\n",
    "print(\"✅ Keras model, scaler, and helper files saved.\")\n",
    "\n",
    "# --- 8. Convert to TensorFlow Lite (for Raspberry Pi) ---\n",
    "print(\"\\n--- Converting Keras model to TensorFlow Lite for edge deployment... ---\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('patient_risk_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"✅ Model converted and saved as 'patient_risk_model.tflite' ({len(tflite_model) / 1024:.2f} KB)\")\n",
    "print(\"This .tflite file is what you would deploy to the Raspberry Pi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a3fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- 1. Load Artifacts ---\n",
    "print(\"--- Loading 51-condition model and artifacts... ---\")\n",
    "try:\n",
    "    model = tf.keras.models.load_model('patient_risk_model.keras')\n",
    "    scaler = joblib.load('data_scaler.joblib')\n",
    "    feature_columns = json.load(open('feature_columns_multitime.json'))\n",
    "    base_diseases = json.load(open('target_diseases_base.json'))\n",
    "    target_diseases_multitime = model.output_names # Get targets directly from model\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"--- ERROR: Could not find a required file: {e.filename} ---\")\n",
    "    print(\"Please ensure you have run Block 4 to train and save the model first.\")\n",
    "    model = None\n",
    "\n",
    "# --- 2. Create Data for a New Hypothetical Patient (with 51-condition data) ---\n",
    "if model:\n",
    "    new_patient_data = pd.DataFrame({\n",
    "        # --- Original Data ---\n",
    "        'age': [72], 'gender': ['male'],\n",
    "        'sbp': [160], 'dbp': [95], 'heart_rate': [95], 'respiratory_rate': [24],\n",
    "        'pain_score': [6], 'height_m': [1.68], 'weight_kg': [80],\n",
    "        'wbc_count': [11], 'hemoglobin': [12], 'hematocrit': [37], 'platelet_count': [210],\n",
    "        'cholesterol_total': [230], 'triglycerides': [160], 'cholesterol_ldl': [150], 'cholesterol_hdl': [35],\n",
    "        'hba1c': [7.0], 'phq2_score': [1], 'gad7_score': [10], 'temperature': [37.2], 'creatinine': [1.4], 'bun': [22],\n",
    "        'sodium': [140], 'potassium': [4.2], 'glucose': [130], 'bilirubin': [0.8], 'spo2': [96],\n",
    "\n",
    "        # --- Original History ---\n",
    "        'history_hypertension': [1], 'history_diabetes': [1], 'history_chf': [1], 'history_copd': [1],\n",
    "        'history_anemia': [0], 'history_hyperlipidemia': [1], 'history_cad': [1], 'history_ckd': [1],\n",
    "        'history_asthma': [0], 'history_pneumonia': [0], 'history_flu': [0], 'history_obesity': [1],\n",
    "        'on_statin': [1], 'on_metformin': [1], 'on_aspirin': [1], 'on_lisinopril': [1],\n",
    "        'history_cabg': [0], 'history_appendectomy': [0],\n",
    "\n",
    "        # --- NEW LABS for 51-condition model ---\n",
    "        'troponin_i': [0.01], 'crp': [45.0], 'lactate': [1.9], 'procalcitonin': [0.3], 'd_dimer': [350],\n",
    "        'urine_wbc': [10], 'urine_nitrite': [0], 'urine_leukocyte': [1], 'stool_occult_blood': [0],\n",
    "        'albumin': [3.8], 'alt': [30], 'ast': [35], 'tsh': [4.0], 'free_t4': [1.1],\n",
    "        'lipase': [80], 'amylase': [75], 'inr': [1.0], 'ptt': [30], 'magnesium': [1.9],\n",
    "        'phosphate': [3.0], 'rheumatoid_factor': [10],\n",
    "\n",
    "        # --- NEW HISTORY/MEDS for 51-condition model ---\n",
    "        'history_afib': [1], 'history_dvt_pe': [0], 'history_thyroid': [1], 'history_gout': [0],\n",
    "        'history_arthritis': [1], 'history_pancreatitis': [0], 'history_gi_bleed': [0], 'history_ibd': [0],\n",
    "        'history_cellulitis': [0], 'history_uti': [1], 'history_seizure': [0], 'history_dementia': [0],\n",
    "        'on_anticoagulant': [1], 'on_beta_blocker': [1], 'on_diuretic': [1], 'on_ccb': [1],\n",
    "        'on_insulin': [0], 'on_ppi': [1], 'on_thyroid_med': [1], 'on_gout_med': [0],\n",
    "        'proc_colonoscopy': [1], 'proc_egd': [1], 'proc_dialysis': [0]\n",
    "    })\n",
    "\n",
    "    # --- 3. PREPARE DATA FOR PREDICTION (Keras Version) ---\n",
    "    new_patient_processed = new_patient_data.copy()\n",
    "\n",
    "    # a. Create trend features (assume stability for a single snapshot)\n",
    "    vitals_to_track = [\n",
    "        'heart_rate', 'sbp', 'dbp', 'respiratory_rate', 'temperature', 'glucose',\n",
    "        'wbc_count', 'hemoglobin', 'creatinine', 'potassium', 'lactate', 'troponin_i'\n",
    "    ]\n",
    "    for vital in vitals_to_track:\n",
    "        if vital in new_patient_processed.columns:\n",
    "            current_val = new_patient_processed[vital].iloc[0]\n",
    "            new_patient_processed[f'{vital}_avg_3hr'] = current_val\n",
    "            new_patient_processed[f'{vital}_std_3hr'] = 0.0\n",
    "\n",
    "    # b. Create other derived features (must match Block 3)\n",
    "    new_patient_processed = pd.get_dummies(new_patient_processed, columns=['gender'], drop_first=True, dtype=int)\n",
    "    safe_height_sq = (new_patient_processed['height_m'].replace(0, np.nan)) ** 2\n",
    "    safe_sbp = new_patient_processed['sbp'].replace(0, np.nan)\n",
    "    safe_creatinine = new_patient_processed['creatinine'].replace(0, np.nan)\n",
    "\n",
    "    new_patient_processed['map'] = ((2 * new_patient_processed['dbp']) + new_patient_processed['sbp']) / 3\n",
    "    new_patient_processed['shock_index'] = new_patient_processed['heart_rate'] / safe_sbp\n",
    "    new_patient_processed['bun_creatinine_ratio'] = new_patient_processed['bun'] / safe_creatinine\n",
    "    new_patient_processed['anion_gap'] = new_patient_processed['sodium'] - (105 + 24) # Using defaults\n",
    "    new_patient_processed['pulse_pressure'] = new_patient_processed['sbp'] - new_patient_processed['dbp']\n",
    "\n",
    "    # c. Create delta features (assume 0 for a snapshot)\n",
    "    delta_cols_list = [\n",
    "        'sbp', 'dbp', 'heart_rate', 'wbc_count', 'hemoglobin', 'map',\n",
    "        'creatinine', 'lactate', 'troponin_i'\n",
    "    ]\n",
    "    for col in delta_cols_list:\n",
    "        new_patient_processed[f'delta_{col}'] = 0\n",
    "\n",
    "    # d. Align columns with the training set\n",
    "    new_patient_processed = new_patient_processed.reindex(columns=feature_columns).fillna(0)\n",
    "\n",
    "    # e. Scale the data using the saved scaler\n",
    "    new_patient_scaled = scaler.transform(new_patient_processed)\n",
    "\n",
    "    # --- 4. GET PREDICTIONS (Keras Version) ---\n",
    "    print(\"--- Making predictions with the Keras multi-task model... ---\")\n",
    "    probabilities_list = model.predict(new_patient_scaled)\n",
    "\n",
    "    results_map = {name: probabilities_list[i][0][0] for i, name in enumerate(target_diseases_multitime)}\n",
    "\n",
    "    # --- 5. DISPLAY RESULTS IN A TABLE ---\n",
    "    output_df_data = []\n",
    "    for disease_base in base_diseases:\n",
    "        clean_name = disease_base.replace('_',' ').title()\n",
    "        output_df_data.append({\n",
    "            \"Condition\": clean_name,\n",
    "            \"6-Hour Risk\": f\"{results_map.get('risk_' + disease_base + '_6h', 0):.2%}\",\n",
    "            \"24-Hour Risk\": f\"{results_map.get('risk_' + disease_base + '_24h', 0):.2%}\",\n",
    "            \"48-Hour Risk\": f\"{results_map.get('risk_' + disease_base + '_48h', 0):.2%}\",\n",
    "            \"_sort_key\": results_map.get('risk_' + disease_base + '_24h', 0)\n",
    "        })\n",
    "\n",
    "    sorted_results = sorted(output_df_data, key=lambda x: x[\"_sort_key\"], reverse=True)\n",
    "    results_df = pd.DataFrame(sorted_results).drop(columns=['_sort_key'])\n",
    "\n",
    "    print(\"\\n--- Multi-Timeframe Predictive Risk Assessment (51 Conditions) ---\")\n",
    "    print(results_df.to_string(max_rows=len(sorted_results)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"--- Evaluating Multi-Task Model Performance on Unseen Test Data ---\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Load Artifacts ---\n",
    "    print(\"Loading model, scaler, and file lists...\")\n",
    "    model = tf.keras.models.load_model('patient_risk_model.keras')\n",
    "    scaler = joblib.load('data_scaler.joblib')\n",
    "\n",
    "    # --- 2. Re-create File Lists and Column Names ---\n",
    "    final_chunks_dir = '/content/final_parquet_chunks/'\n",
    "    all_chunk_files = [os.path.join(final_chunks_dir, f) for f in os.listdir(final_chunks_dir) if f.endswith('.parquet')]\n",
    "    all_chunk_files.sort()\n",
    "\n",
    "    sample_df = pd.read_parquet(all_chunk_files[0])\n",
    "    target_diseases_multitime = [col for col in sample_df.columns if col.startswith('risk_')]\n",
    "    feature_columns = [col for col in sample_df.columns if col not in target_diseases_multitime and col not in ['patient_id', 'obs_time', 'obs_time_utc']]\n",
    "    del sample_df\n",
    "\n",
    "    train_files, val_files = train_test_split(all_chunk_files, test_size=0.2, random_state=42)\n",
    "\n",
    "    # --- 3. Re-create Data Generator (for validation) ---\n",
    "    def data_generator(file_list, scaler, feature_cols, target_cols, batch_size):\n",
    "        while True: # Keras needs this to be infinite\n",
    "            for f in file_list: # No need to shuffle for evaluation\n",
    "                df = pd.read_parquet(f)\n",
    "\n",
    "                for i in range(0, len(df), batch_size):\n",
    "                    batch_df = df.iloc[i:i+batch_size]\n",
    "                    X = batch_df[feature_cols]\n",
    "                    y = batch_df[target_cols]\n",
    "\n",
    "                    X_scaled = scaler.transform(X)\n",
    "                    y_dict = {col: y[col] for col in y.columns}\n",
    "\n",
    "                    yield X_scaled, y_dict\n",
    "\n",
    "                del df\n",
    "                gc.collect()\n",
    "\n",
    "    # --- 4. Evaluate the Model ---\n",
    "    BATCH_SIZE = 256 # Use the same batch size as training\n",
    "\n",
    "    # Calculate total validation rows\n",
    "    val_rows = sum([pq.ParquetFile(f).metadata.num_rows for f in val_files])\n",
    "    validation_steps = math.ceil(val_rows / BATCH_SIZE)\n",
    "\n",
    "    print(f\"\\nFound {val_rows} validation rows. Using {validation_steps} steps.\")\n",
    "\n",
    "    # Re-create the generator\n",
    "    val_gen = data_generator(val_files, scaler, feature_columns, target_diseases_multitime, BATCH_SIZE)\n",
    "\n",
    "    print(\"Running model.evaluate()...\")\n",
    "    # This runs the model over all validation batches\n",
    "    results = model.evaluate(\n",
    "        val_gen,\n",
    "        steps=validation_steps,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # --- 5. Display Results ---\n",
    "    print(\"\\n--- Evaluation Complete ---\")\n",
    "\n",
    "    # Keras returns a list: [total_loss, head_1_loss, head_2_loss, ..., head_1_auc, head_2_auc, ...]\n",
    "    # We can map them using the model's metric names\n",
    "    results_map = dict(zip(model.metrics_names, results))\n",
    "\n",
    "    # Filter just for the AUC scores\n",
    "    auc_scores = {name: score for name, score in results_map.items() if 'auc' in name}\n",
    "\n",
    "    if auc_scores:\n",
    "        # Sort by AUC score, descending\n",
    "        sorted_auc = sorted(auc_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        print(f\"✅ Average AUC-ROC across all {len(auc_scores)} outputs: {np.mean(list(auc_scores.values())):.4f}\\n\")\n",
    "\n",
    "        print(\"--- Best Performing Predictions (Top 10) ---\")\n",
    "        for name, score in sorted_auc[:10]:\n",
    "            print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "        print(\"\\n--- Worst Performing Predictions (Bottom 10) ---\")\n",
    "        for name, score in sorted_auc[-10:]:\n",
    "            print(f\"{name}: {score:.4f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Could not parse AUC scores from evaluation results.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"--- ERROR during evaluation: {e} ---\")\n",
    "    print(\"Please ensure Block 4 has been run successfully and all files are present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. INSTALL GRADIO (if not already installed) ---\n",
    "!pip install gradio -q\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# --- 2. LOAD SAVED ARTIFACTS ---\n",
    "print(\"--- Loading 51-condition model, scaler, and helper files... ---\")\n",
    "try:\n",
    "    model = tf.keras.models.load_model('patient_risk_model.keras')\n",
    "    scaler = joblib.load('data_scaler.joblib')\n",
    "    feature_columns = json.load(open('feature_columns_multitime.json'))\n",
    "    base_diseases = json.load(open('target_diseases_base.json'))\n",
    "    target_diseases_multitime = model.output_names\n",
    "\n",
    "    # Create a simple median map for imputation (using scaler's mean)\n",
    "    # This is more robust than the old median file\n",
    "    imputation_values = pd.Series(scaler.mean_, index=feature_columns)\n",
    "\n",
    "    print(\"✅ Keras model and all necessary files loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"--- ERROR: Could not load a required file. {e} ---\")\n",
    "    model, scaler, feature_columns, base_diseases, target_diseases_multitime = None, None, [], [], []\n",
    "    imputation_values = pd.Series()\n",
    "\n",
    "# --- 3. CREATE THE PREDICTION FUNCTION (51-Condition Version) ---\n",
    "def predict_risk_multitime(ignore_features, *args):\n",
    "    if model is None: return pd.DataFrame({\"Error\": [\"Model not loaded. Please train the model first.\"]})\n",
    "\n",
    "    # Map all the Gradio inputs back to a dictionary\n",
    "    input_names = [inp.label for inp in all_inputs if inp.label and inp.label != \"Ignore Inputs\"]\n",
    "    input_values = dict(zip(input_names, args))\n",
    "\n",
    "    # Use a copy of the imputation values (scaler means)\n",
    "    input_data = imputation_values.copy()\n",
    "\n",
    "    # Overwrite the means with user-provided data\n",
    "    for name, value in input_values.items():\n",
    "        # Generate the feature key\n",
    "        # --- BUG FIX: Completed the replace() chain ---\n",
    "        key = name.lower().replace(' ', '_').replace('(', '').replace(')', '').replace('°c', '').replace('/', '_').replace('-', '_')\n",
    "        if key == 'systolic_bp': key = 'sbp'\n",
    "        if key == 'diastolic_bp': key = 'dbp'\n",
    "        if key == 'temperature_c': key = 'temperature'\n",
    "\n",
    "        # Use imputation value if the field is blank (None) OR if it was checked\n",
    "        if value is None or name in ignore_features:\n",
    "            # We're already using the mean, so just 'continue'\n",
    "            continue\n",
    "        else:\n",
    "            # User provided a value, so overwrite the mean\n",
    "            if key in input_data:\n",
    "                input_data[key] = value\n",
    "\n",
    "    # --- Handle Checkboxes for History and Medications ---\n",
    "    # args[-5] = medical_history\n",
    "    # args[-4] = surgical_history\n",
    "    # args[-3] = current_meds\n",
    "    # args[-2] = diuretic_meds\n",
    "    # args[-1] = other_meds\n",
    "\n",
    "    hist_med_map = {\n",
    "        'Hypertension': 'history_hypertension', 'Diabetes': 'history_diabetes', 'CHF': 'history_chf',\n",
    "        'COPD': 'history_copd', 'Anemia': 'history_anemia', 'Hyperlipidemia': 'history_hyperlipidemia',\n",
    "        'CAD': 'history_cad', 'CKD': 'history_ckd', 'Asthma': 'history_asthma', 'Atrial Fibrillation': 'history_afib',\n",
    "        'PE/DVT History': 'history_dvt_pe', 'Thyroid Disease': 'history_thyroid', 'Gout': 'history_gout',\n",
    "        'Arthritis': 'history_arthritis', 'Pancreatitis': 'history_pancreatitis', 'GI Bleed': 'history_gi_bleed',\n",
    "        'IBD (Crohn\\'s/Colitis)': 'history_ibd', 'Cellulitis': 'history_cellulitis', 'UTI': 'history_uti',\n",
    "        'Seizure': 'history_seizure', 'Dementia': 'history_dementia'\n",
    "    }\n",
    "    for choice, key in hist_med_map.items():\n",
    "        input_data[key] = 1 if choice in args[-5] else 0\n",
    "\n",
    "    proc_map = {'CABG': 'history_cabg', 'Appendectomy': 'history_appendectomy', 'Colonoscopy': 'proc_colonoscopy', 'EGD': 'proc_egd', 'Dialysis': 'proc_dialysis'}\n",
    "    for choice, key in proc_map.items():\n",
    "        input_data[key] = 1 if choice in args[-4] else 0\n",
    "\n",
    "    med_map_1 = {'Statin': 'on_statin', 'Metformin': 'on_metformin', 'Aspirin': 'on_aspirin', 'Lisinopril (ACE-I)': 'on_lisinopril', 'Insulin': 'on_insulin'}\n",
    "    for choice, key in med_map_1.items():\n",
    "        input_data[key] = 1 if choice in args[-3] else 0\n",
    "\n",
    "    med_map_2 = {'Furosemide': 'on_diuretic', 'Hydrochlorothiazide': 'on_diuretic', 'Spironolactone': 'on_diuretic'}\n",
    "    input_data['on_diuretic'] = 1 if any(choice in args[-2] for choice in med_map_2.keys()) else 0\n",
    "\n",
    "    med_map_3 = {\n",
    "        'Anticoagulant (Warfarin/Eliquis/Xarelto)': 'on_anticoagulant', 'Beta Blocker (Metoprolol/Atenolol)': 'on_beta_blocker',\n",
    "        'Calcium Channel Blocker (Amlodipine)': 'on_ccb', 'PPI (Omeprazole/Pantoprazole)': 'on_ppi',\n",
    "        'Levothyroxine': 'on_thyroid_med', 'Allopurinol': 'on_gout_med'\n",
    "    }\n",
    "    for choice, key in med_map_3.items():\n",
    "        input_data[key] = 1 if choice in args[-1] else 0\n",
    "\n",
    "\n",
    "    df = pd.DataFrame([input_data])\n",
    "\n",
    "    # --- Re-create Derived Features (must match Block 3) ---\n",
    "    # Note: We are overwriting the imputed values with more accurate calculations\n",
    "    safe_height_sq = (df['height_m'].replace(0, np.nan)) ** 2\n",
    "    safe_sbp = df['sbp'].replace(0, np.nan)\n",
    "    safe_creatinine = df['creatinine'].replace(0, np.nan)\n",
    "\n",
    "    df['map'] = ((2 * df['dbp']) + df['sbp']) / 3\n",
    "    df['shock_index'] = df['heart_rate'] / safe_sbp\n",
    "    df['bun_creatinine_ratio'] = df['bun'] / safe_creatinine\n",
    "    df['anion_gap'] = df['sodium'] - (105 + 24) # Using defaults\n",
    "    df['pulse_pressure'] = df['sbp'] - df['dbp']\n",
    "\n",
    "    # Impute trend features (assume stability for snapshot prediction)\n",
    "    vitals_to_track = [\n",
    "        'heart_rate', 'sbp', 'dbp', 'respiratory_rate', 'temperature', 'glucose',\n",
    "        'wbc_count', 'hemoglobin', 'creatinine', 'potassium', 'lactate', 'troponin_i'\n",
    "    ]\n",
    "    for vital in vitals_to_track:\n",
    "        if vital in df.columns:\n",
    "            current_val = df[vital].iloc[0]\n",
    "            # --- BUG FIX: Fixed typo 'current_al' to 'current_val' ---\n",
    "            df[f'{vital}_avg_3hr'] = current_val\n",
    "            df[f'{vital}_std_3hr'] = 0.0\n",
    "\n",
    "    # Impute delta features (assume 0 for a snapshot)\n",
    "    delta_cols_list = ['sbp', 'dbp', 'heart_rate', 'wbc_count', 'hemoglobin', 'map', 'creatinine', 'lactate', 'troponin_i']\n",
    "    for col in delta_cols_list:\n",
    "        if col in feature_columns:\n",
    "            df[f'delta_{col}'] = 0\n",
    "\n",
    "    # Align columns, fill any *new* NaNs created by derived features\n",
    "    df = df.reindex(columns=feature_columns).fillna(0)\n",
    "\n",
    "    # --- Scale the data ---\n",
    "    df_scaled = scaler.transform(df)\n",
    "\n",
    "    # --- Make Predictions ---\n",
    "    probabilities_list = model.predict(df_scaled)\n",
    "    results_map = {name: probabilities_list[i][0][0] for i, name in enumerate(target_diseases_multitime)}\n",
    "\n",
    "    # Format the output table\n",
    "    output_df_data = []\n",
    "    for disease_base in base_diseases:\n",
    "        clean_name = disease_base.replace('_',' ').title()\n",
    "        output_df_data.append({\n",
    "            \"Condition\": clean_name,\n",
    "            \"6-Hour Risk\": f\"{results_map.get('risk_' + disease_base + '_6h', 0):.2%}\",\n",
    "            \"24-Hour Risk\": f\"{results_map.get('risk_' + disease_base + '_24h', 0):.2%}\",\n",
    "            \"48-Hour Risk\": f\"{results_map.get('risk_' + disease_base + '_48h', 0):.2%}\",\n",
    "            \"_sort_key\": results_map.get('risk_' + disease_base + '_24h', 0)\n",
    "        })\n",
    "    sorted_results = sorted(output_df_data, key=lambda x: x[\"_sort_key\"], reverse=True)\n",
    "    return pd.DataFrame(sorted_results).drop(columns=['_sort_key'])\n",
    "\n",
    "# --- 4. DEFINE THE GRADIO INTERFACE (51-Condition Version) ---\n",
    "# Create the list of all labels for the \"Ignore\" box\n",
    "vitals_labels = [ \"Systolic BP\", \"Diastolic BP\", \"Heart Rate\", \"Respiratory Rate\", \"Temperature (C)\", \"Pain Score (0-10)\", \"Height (m)\", \"Weight (kg)\", \"SpO2\"]\n",
    "core_labs_labels = [\"WBC Count\", \"Hemoglobin\", \"Hematocrit\", \"Platelet Count\", \"Glucose\", \"Creatinine\", \"BUN\", \"Sodium\", \"Potassium\", \"Bilirubin\"]\n",
    "lipid_panel_labels = [\"Total Cholesterol\", \"Triglycerides\", \"LDL Cholesterol\", \"HDL Cholesterol\"]\n",
    "# --- NEW LABS ---\n",
    "new_labs_labels = [\n",
    "    \"Troponin I\", \"CRP\", \"Lactate\", \"Procalcitonin\", \"D-dimer\", \"Urine WBC\", \"Urine Nitrite\",\n",
    "    \"Urine Leukocyte\", \"Stool Occult Blood\", \"Albumin\", \"ALT\", \"AST\", \"TSH\", \"Free T4\",\n",
    "    \"Lipase\", \"Amylase\", \"INR\", \"PTT\", \"Magnesium\", \"Phosphate\", \"Rheumatoid Factor\"\n",
    "]\n",
    "scores_labels = [\"HbA1c\", \"PHQ-2 Score\", \"GAD-7 Score\"]\n",
    "numerical_inputs_labels = vitals_labels + core_labs_labels + lipid_panel_labels + new_labs_labels + scores_labels\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Patient Multi-Timeframe Risk Prediction (51 Conditions)\")\n",
    "    gr.Markdown(\"Enter patient data. Check a box in 'Ignore Inputs' to use an imputed value (the dataset mean) for any field you don't have data for.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        ignore_box = gr.CheckboxGroup(choices=numerical_inputs_labels, label=\"Ignore Inputs\", scale=1)\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            with gr.Row():\n",
    "                age = gr.Number(label=\"Age\")\n",
    "                gender = gr.Radio(label=\"Gender\", choices=[\"male\", \"female\"], value=\"male\")\n",
    "\n",
    "            with gr.Accordion(\"Vitals & Measurements\", open=False):\n",
    "                with gr.Row():\n",
    "                    sbp = gr.Number(label=\"Systolic BP\")\n",
    "                    dbp = gr.Number(label=\"Diastolic BP\")\n",
    "                    heart_rate = gr.Number(label=\"Heart Rate\")\n",
    "                    respiratory_rate = gr.Number(label=\"Respiratory Rate\")\n",
    "                with gr.Row():\n",
    "                    temperature = gr.Number(label=\"Temperature (C)\")\n",
    "                    spo2 = gr.Number(label=\"SpO2\")\n",
    "                    pain_score = gr.Number(label=\"Pain Score (0-10)\")\n",
    "                with gr.Row():\n",
    "                    height_m = gr.Number(label=\"Height (m)\")\n",
    "                    weight_kg = gr.Number(label=\"Weight (kg)\")\n",
    "\n",
    "            with gr.Accordion(\"Core Labs\", open=False):\n",
    "                with gr.Row():\n",
    "                    wbc_count = gr.Number(label=\"WBC Count\")\n",
    "                    hemoglobin = gr.Number(label=\"Hemoglobin\")\n",
    "                    hematocrit = gr.Number(label=\"Hematocrit\")\n",
    "                    platelet_count = gr.Number(label=\"Platelet Count\")\n",
    "                with gr.Row():\n",
    "                    glucose = gr.Number(label=\"Glucose\")\n",
    "                    creatinine = gr.Number(label=\"Creatinine\")\n",
    "                    bun = gr.Number(label=\"BUN\")\n",
    "                with gr.Row():\n",
    "                    sodium = gr.Number(label=\"Sodium\")\n",
    "                    potassium = gr.Number(label=\"Potassium\")\n",
    "                    bilirubin = gr.Number(label=\"Bilirubin\")\n",
    "\n",
    "            # --- NEW LABS SECTION ---\n",
    "            with gr.Accordion(\"Specialty Labs (Cardiac, Inflammatory, Coag, ...)\", open=False):\n",
    "                with gr.Row():\n",
    "                    troponin_i = gr.Number(label=\"Troponin I\")\n",
    "                    crp = gr.Number(label=\"CRP\")\n",
    "                    lactate = gr.Number(label=\"Lactate\")\n",
    "                    procalcitonin = gr.Number(label=\"Procalcitonin\")\n",
    "                with gr.Row():\n",
    "                    d_dimer = gr.Number(label=\"D-dimer\")\n",
    "                    inr = gr.Number(label=\"INR\")\n",
    "                    ptt = gr.Number(label=\"PTT\")\n",
    "                with gr.Row():\n",
    "                    albumin = gr.Number(label=\"Albumin\")\n",
    "                    alt = gr.Number(label=\"ALT\")\n",
    "                    ast = gr.Number(label=\"AST\")\n",
    "                with gr.Row():\n",
    "                    lipase = gr.Number(label=\"Lipase\")\n",
    "                    amylase = gr.Number(label=\"Amylase\")\n",
    "                    rheumatoid_factor = gr.Number(label=\"Rheumatoid Factor\")\n",
    "                with gr.Row():\n",
    "                    magnesium = gr.Number(label=\"Magnesium\")\n",
    "                    phosphate = gr.Number(label=\"Phosphate\")\n",
    "\n",
    "            with gr.Accordion(\"Lipid Panel, Thyroid & Urinalysis\", open=False):\n",
    "                with gr.Row():\n",
    "                    cholesterol_total = gr.Number(label=\"Total Cholesterol\")\n",
    "                    triglycerides = gr.Number(label=\"Triglycerides\")\n",
    "                    cholesterol_ldl = gr.Number(label=\"LDL Cholesterol\")\n",
    "                    cholesterol_hdl = gr.Number(label=\"HDL Cholesterol\")\n",
    "                with gr.Row():\n",
    "                    tsh = gr.Number(label=\"TSH\")\n",
    "                    free_t4 = gr.Number(label=\"Free T4\")\n",
    "                    stool_occult_blood = gr.Number(label=\"Stool Occult Blood\")\n",
    "                with gr.Row():\n",
    "                    urine_wbc = gr.Number(label=\"Urine WBC\")\n",
    "                    urine_nitrite = gr.Number(label=\"Urine Nitrite\")\n",
    "                    urine_leukocyte = gr.Number(label=\"Urine Leukocyte\")\n",
    "\n",
    "            with gr.Accordion(\"Scores & History\", open=False):\n",
    "                with gr.Row():\n",
    "                    hba1c = gr.Number(label=\"HbA1c\")\n",
    "                    phq2_score = gr.Number(label=\"PHQ-2 Score\")\n",
    "                    gad7_score = gr.Number(label=\"GAD-7 Score\")\n",
    "\n",
    "                # --- NEW HISTORY/MEDS ---\n",
    "                medical_history = gr.CheckboxGroup(label=\"Medical History\", choices=[\n",
    "                    \"Hypertension\", \"Diabetes\", \"CHF\", \"COPD\", \"Anemia\", \"Hyperlipidemia\", \"CAD\", \"CKD\", \"Asthma\",\n",
    "                    \"Atrial Fibrillation\", \"PE/DVT History\", \"Thyroid Disease\", \"Gout\", \"Arthritis\",\n",
    "                    \"Pancreatitis\", \"GI Bleed\", \"IBD (Crohn's/Colitis)\", \"Cellulitis\", \"UTI\", \"Seizure\", \"Dementia\"\n",
    "                ], value=[]) # Set default to empty list\n",
    "                surgical_history = gr.CheckboxGroup(label=\"Surgical/Procedure History\", choices=[\n",
    "                    \"CABG\", \"Appendectomy\", \"Colonoscopy\", \"EGD\", \"Dialysis\"\n",
    "                ], value=[])\n",
    "                current_meds = gr.CheckboxGroup(label=\"Current Medications (Common)\", choices=[\n",
    "                    \"Statin\", \"Metformin\", \"Aspirin\", \"Lisinopril (ACE-I)\", \"Insulin\"\n",
    "                ], value=[])\n",
    "                diuretic_meds = gr.CheckboxGroup(label=\"Current Medications (Diuretics)\", choices=[\n",
    "                    \"Furosemide\", \"Hydrochlorothiazide\", \"Spironolactone\"\n",
    "                ], value=[])\n",
    "                other_meds = gr.CheckboxGroup(label=\"Current Medications (Other)\", choices=[\n",
    "                    \"Anticoagulant (Warfarin/Eliquis/Xarelto)\", \"Beta Blocker (Metoprolol/Atenolol)\",\n",
    "                    \"Calcium Channel Blocker (Amlodipine)\", \"PPI (Omeprazole/Pantoprazole)\",\n",
    "                    \"Levothyroxine\", \"Allopurinol\"\n",
    "                ], value=[])\n",
    "\n",
    "    submit_btn = gr.Button(\"Predict Risk\")\n",
    "    output = gr.Dataframe(headers=[\"Condition\", \"6-Hour Risk\", \"24-Hour Risk\", \"48-Hour Risk\"], label=\"Predicted Condition Risks\", wrap=True, row_count=51)\n",
    "    # --- UPDATED all_inputs list ---\n",
    "    all_inputs = [\n",
    "        ignore_box, age, gender,\n",
    "        sbp, dbp, heart_rate, respiratory_rate, temperature, spo2, pain_score, height_m, weight_kg,\n",
    "        wbc_count, hemoglobin, hematocrit, platelet_count, glucose, creatinine, bun, sodium, potassium, bilirubin,\n",
    "        troponin_i, crp, lactate, procalcitonin, d_dimer, inr, ptt, albumin, alt, ast, lipase, amylase, rheumatoid_factor, magnesium, phosphate,\n",
    "        cholesterol_total, triglycerides, cholesterol_ldl, cholesterol_hdl,\n",
    "        tsh, free_t4, stool_occult_blood, urine_wbc, urine_nitrite, urine_leukocyte,\n",
    "        hba1c, phq2_score, gad7_score,\n",
    "        medical_history, surgical_history, current_meds, diuretic_meds, other_meds\n",
    "    ]\n",
    "\n",
    "    submit_btn.click(fn=predict_risk_multitime, inputs=all_inputs, outputs=output)\n",
    "\n",
    "# --- 5. LAUNCH THE GUI ---\n",
    "if model:\n",
    "    demo.launch(share=True, debug=True)\n",
    "else:\n",
    "    print(\"Gradio app not launched because the model failed to load.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
